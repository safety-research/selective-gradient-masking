{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "from sgtm.model import GPTNeoForCausalLMSGTM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "sns.set_palette(\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35185972",
   "metadata": {},
   "outputs": [],
   "source": "ES_DS_PATH = Path(\"data/datasets/tinystories_split/es\")\nEN_DS_PATH = Path(\"data/datasets/tinystories_split/en\")\nBASE_MODEL_DIR = Path(\"data/models\")\n\nMODEL_PATH = \"your-model-name\"\nmodel = GPTNeoForCausalLMSGTM.from_pretrained(BASE_MODEL_DIR / MODEL_PATH / \"output\"/ \"final-checkpoint\").to(\"cuda\")\n\nes_ds = load_from_disk(ES_DS_PATH)\nen_ds = load_from_disk(EN_DS_PATH)\n\ntokenizer = GPT2Tokenizer.from_pretrained(BASE_MODEL_DIR / MODEL_PATH / \"output\"/ \"final-checkpoint\")\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_dataloader = DataLoader(\n",
    "    es_ds[\"test\"].select_columns([\"input_ids\", \"attention_mask\"]),\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "en_dataloader = DataLoader(\n",
    "    en_ds[\"test\"].select_columns([\"input_ids\", \"attention_mask\"]),\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"es\": es_dataloader,\n",
    "    \"en\": en_dataloader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameter norms for retain and forget parameters\n",
    "print(\"Parameter Norms:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate full parameter norms\n",
    "forget_param_norm = 0.0\n",
    "retain_param_norm = 0.0\n",
    "forget_param_count = 0\n",
    "retain_param_count = 0\n",
    "\n",
    "for name, param in model.named_parameters_split(sgtm_split=\"forget\"):\n",
    "    param_norm = param.norm().item()\n",
    "    forget_param_norm += param_norm ** 2\n",
    "    forget_param_count += param.numel()\n",
    "\n",
    "for name, param in model.named_parameters_split(sgtm_split=\"retain\"):\n",
    "    param_norm = param.norm().item()\n",
    "    retain_param_norm += param_norm ** 2\n",
    "    retain_param_count += param.numel()\n",
    "\n",
    "param_norms = {}\n",
    "param_norms[\"forget\"] = forget_param_norm ** 0.5\n",
    "param_norms[\"retain\"] = retain_param_norm ** 0.5\n",
    "\n",
    "# Calculate normalized parameter norms\n",
    "forget_param_norm_normalized = param_norms[\"forget\"] / (forget_param_count ** 0.5)\n",
    "retain_param_norm_normalized = param_norms[\"retain\"] / (retain_param_count ** 0.5)\n",
    "\n",
    "print(f\"Forget parameter norm (full): {param_norms['forget']:.6f}\")\n",
    "print(f\"Retain parameter norm (full): {param_norms['retain']:.6f}\")\n",
    "print(f\"Forget parameter norm (normalized): {forget_param_norm_normalized:.6f}\")\n",
    "print(f\"Retain parameter norm (normalized): {retain_param_norm_normalized:.6f}\")\n",
    "print(f\"Forget parameter count: {forget_param_count}\")\n",
    "print(f\"Retain parameter count: {retain_param_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc071086",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = defaultdict(list)\n",
    "normnorms = defaultdict(list)\n",
    "props = defaultdict(list)\n",
    "\n",
    "for lang, dataloader in dataloaders.items():\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        with torch.amp.autocast(\"cuda\", enabled=True, dtype=torch.bfloat16):\n",
    "            outputs = model(return_dict=True, **batch)\n",
    "\n",
    "        outputs.loss.backward()\n",
    "\n",
    "        for param_group in (\"forget\", \"retain\"):\n",
    "            grad_norm = 0.0\n",
    "            param_count = 0\n",
    "\n",
    "            for name, param in model.named_parameters_split(sgtm_split=param_group):\n",
    "                if param.grad is not None:\n",
    "                    param_norm = param.grad.detach().data.norm(2)\n",
    "                    grad_norm += param_norm.item() ** 2\n",
    "                    param_count += param.numel()\n",
    "\n",
    "            norms[(lang, param_group)].append(grad_norm ** 0.5)\n",
    "            normnorms[(lang, param_group)].append( (grad_norm ** 0.5) / param_norms[param_group])\n",
    "\n",
    "\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data as pickle\n",
    "with open(\"data/grad_norms_raw.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"norms\": dict(norms),\n",
    "        \"normnorms\": dict(normnorms), \n",
    "        \"props\": dict(props),\n",
    "        \"param_norms\": param_norms,\n",
    "        \"param_counts\": {\"forget\": forget_param_count, \"retain\": retain_param_count}\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e592e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from pickle\n",
    "with open(\"data/grad_norms_raw.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    norms = data[\"norms\"]\n",
    "    normnorms = data[\"normnorms\"]\n",
    "    props = data[\"props\"]\n",
    "    param_norms = data[\"param_norms\"]\n",
    "    param_counts = data[\"param_counts\"]\n",
    "    forget_param_count = param_counts[\"forget\"]\n",
    "    retain_param_count = param_counts[\"retain\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out p99 outliers from all metrics\n",
    "for key in list(norms.keys()):\n",
    "    values = np.array(norms[key])\n",
    "    p99 = np.percentile(values, 99)\n",
    "    filtered_values = values[values <= p99]\n",
    "    norms[key] = filtered_values.tolist()\n",
    "\n",
    "for key in list(normnorms.keys()):\n",
    "    values = np.array(normnorms[key])\n",
    "    p99 = np.percentile(values, 99)\n",
    "    filtered_values = values[values <= p99]\n",
    "    normnorms[key] = filtered_values.tolist()\n",
    "\n",
    "for key in list(props.keys()):\n",
    "    values = np.array(props[key])\n",
    "    p99 = np.percentile(values, 99)\n",
    "    filtered_values = values[values <= p99]\n",
    "    props[key] = filtered_values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d13eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "sns.set_palette(\"muted\")\n",
    "param_colors = {\n",
    "    'forget': 'C1',\n",
    "    'retain': 'C0',\n",
    "}\n",
    "\n",
    "plot_order = [('en', 'forget'), ('en', 'retain'), ('es', 'forget'), ('es', 'retain')]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for lang, param_group in plot_order:\n",
    "    k = (lang, param_group)\n",
    "    v = normnorms[k]\n",
    "    \n",
    "    color = param_colors[param_group]\n",
    "    \n",
    "    kde = gaussian_kde(v)\n",
    "    x_range = np.linspace(min(v), max(v), 200)\n",
    "    density = kde(x_range)\n",
    "    \n",
    "    linestyle = '--' if lang == 'en' else '-'\n",
    "    alpha = 0.2 if lang == 'en' else 0.4\n",
    "    \n",
    "    plt.plot(x_range, density, color=color, linewidth=3, linestyle=linestyle)\n",
    "    plt.fill_between(x_range, density, alpha=alpha, color=color)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='none', edgecolor='none', label='Data:'),\n",
    "    Line2D([0], [0], color='gray', linestyle='--', linewidth=4, label='  Retain (English)'),\n",
    "    Line2D([0], [0], color='gray', linestyle='-', linewidth=4, label='  Forget (Spanish)'),\n",
    "    Line2D([0], [0], color='none', linewidth=0, label=''),  # Spacer\n",
    "    Patch(facecolor='none', edgecolor='none', label='Parameters:'),\n",
    "    Line2D([0], [0], color=param_colors['forget'], linewidth=5, label='  Forget'),\n",
    "    Line2D([0], [0], color=param_colors['retain'], linewidth=5, label='  Retain'),\n",
    "]\n",
    "\n",
    "plt.ylim(0, 450)\n",
    "plt.xlim(0.0025, 0.025)\n",
    "plt.xlabel('Normalized Gradient Norm', fontsize=16)\n",
    "plt.ylabel('Density', fontsize=16)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(handles=legend_elements, fontsize=16, framealpha=0.9, \n",
    "           handlelength=1.5, handletextpad=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "param_colors = {\n",
    "    'forget': 'C0',\n",
    "    'retain': 'C2',\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "gs = GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "axes = [\n",
    "    fig.add_subplot(gs[0, 0]),\n",
    "    fig.add_subplot(gs[0, 1]),\n",
    "    fig.add_subplot(gs[0, 2]),\n",
    "    fig.add_subplot(gs[0, 3]),\n",
    "]\n",
    "\n",
    "plot_order = [('en', 'forget'), ('en', 'retain'), ('es', 'forget'), ('es', 'retain')]\n",
    "\n",
    "def plot(ax, lang, param_group):\n",
    "    k = (lang, param_group)\n",
    "    v = normnorms[k]\n",
    "    color = param_colors[param_group]\n",
    "    \n",
    "    # Create KDE\n",
    "    kde = gaussian_kde(v)\n",
    "    x_range = np.linspace(min(v), max(v), 200)\n",
    "    density = kde(x_range)\n",
    "    \n",
    "    linestyle = '--' if lang == 'en' else '-'\n",
    "    alpha = 0.2 if lang == 'en' else 0.4\n",
    "    \n",
    "    ax.plot(x_range, density, color=color, linewidth=3, linestyle=linestyle)\n",
    "    ax.fill_between(x_range, density, alpha=alpha, color=color)\n",
    "\n",
    "def plot_hist(ax, lang, param_group):\n",
    "    k = (lang, param_group)\n",
    "    v = normnorms[k]\n",
    "    color = param_colors[param_group]\n",
    "    \n",
    "    alpha = 0.3 if lang == 'en' else 0.6\n",
    "    ax.hist(v, bins=30, color=color, alpha=alpha, density=True, \n",
    "            histtype='stepfilled', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=param_colors['forget'], linewidth=2, label='Forget weights'),\n",
    "    Line2D([0], [0], color=param_colors['retain'], linewidth=2, label='Retain weights'),\n",
    "]\n",
    "\n",
    "axes[0].set_title(\"Forget data\", fontsize=18)\n",
    "plot(axes[0], \"es\", \"forget\")\n",
    "plot(axes[0], \"es\", \"retain\")\n",
    "axes[0].legend(handles=legend_elements, fontsize=14)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=param_colors['forget'], linestyle='--', linewidth=2, label='Forget weights'),\n",
    "    Line2D([0], [0], color=param_colors['retain'], linestyle='--', linewidth=2, label='Retain weights'),\n",
    "]\n",
    "axes[1].set_title(\"Retain data\", fontsize=18)\n",
    "plot(axes[1], \"en\", \"forget\")\n",
    "plot(axes[1], \"en\", \"retain\")\n",
    "axes[1].legend(handles=legend_elements, fontsize=14)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=param_colors['forget'], linestyle='--', linewidth=2, label='Retain data'),\n",
    "    Line2D([0], [0], color=param_colors['forget'], linestyle='-', linewidth=2, label='Forget data'),\n",
    "]\n",
    "axes[2].set_title(\"Forget weights\", fontsize=18)\n",
    "plot(axes[2], \"es\", \"forget\")\n",
    "plot(axes[2], \"en\", \"forget\")\n",
    "axes[2].legend(handles=legend_elements, fontsize=14)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=param_colors['retain'], linestyle='--', linewidth=2, label='Retain data'),\n",
    "    Line2D([0], [0], color=param_colors['retain'], linestyle='-', linewidth=2, label='Forget data'),\n",
    "]\n",
    "axes[3].set_title(\"Retain weights\", fontsize=18)\n",
    "plot(axes[3], \"es\", \"retain\")\n",
    "plot(axes[3], \"en\", \"retain\")\n",
    "axes[3].legend(handles=legend_elements, fontsize=14)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0,420)\n",
    "    ax.set_xlim(0.003,0.03)\n",
    "    ax.set_xlabel(\"Relative gradient norm\", fontsize=14)\n",
    "    ax.set_ylabel(\"Density\", fontsize=14)\n",
    "    ax.locator_params(axis='x', nbins=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}